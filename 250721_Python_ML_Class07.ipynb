{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkiKn9RwVZtxqLwkXKFLBT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kheit9527/2025_TCFST_Python_ML/blob/main/250721_Python_ML_Class07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "中小型模型(我們學習的目標) v.s. LLM(大型語言模型，乃至於AGI通用人工智慧<問什麼都能答>)\n",
        "深度學習目標: 模仿人類神經\n",
        "接受刺激 -> 彙總再一起 -> 傳到下一個神經元\n",
        "(接收 -> 合併 -> 輸出訊號)\n",
        "\n",
        "\n",
        "全有全無率: (類似膝跳反應、靜/動摩擦力)\n",
        "    到一個臨界值後自動反映\n",
        "    神經沒有連續概念(只有\"有\"or\"沒有)\n",
        "    門檻: Threshold(幾乎是瞬間，但還是有上升過程)\n",
        "    Ex. 吃辣之前沒有程度之分，吃了才有感受到辣\n",
        "\n",
        "x1 -> w1 (加權值)\n",
        "x2 -> w2\n",
        "-> w1x1 + w2x2 (激活函式)\n",
        "\n",
        "Activation(激活函式, 超過數值才\"激活\", 否則沒激活)\n",
        "\n",
        "神經套用到分類問題? -> 可以\n",
        "\n",
        "Perceptron(感知器): 二維砍一刀直線、三維砍出一個面\n",
        "\n",
        "線性分類器問題: 如果分類器只能做為功能，則會越難分類\n",
        "\n",
        "線性分類不能做日常生活問題 -> 該怎麼做可以讓Perceptron不是切一條直線(侷限性太大)\n",
        "\n",
        "Perceptron改轉換函式: \"Sigmoid函式\"\n",
        "幾乎瞬間爬升 -> S型爬升(門檻值=0.5)\n",
        "數學式: y = 1/1+e^(-x)\n",
        "        if x = 0, y = 0.5\n",
        "        x -> 無限, y趨近於1\n",
        "        x -> 負無限, y趨近於0\n",
        "稱為Logistic Regression\n",
        "(羅吉斯迴歸<還是一種分類, 可能性依舊只有兩種(活or不活)>)\n",
        "\n",
        "Perceptron可以做多元分類(看你的東西離哪一個門檻值近，以鳶尾花為例, 是/不是某一種)\n",
        "\n",
        "線性分類 v.s. 非線性分類\n",
        "決策樹? -> 非線性分類(雖然每一刀都是線性, 但不只砍了一刀, 砍很多刀某種程度上也是有曲線的感覺)\n",
        "\n",
        "@Case: Titanic: 用原始資料產出更多欄位\n",
        "\n",
        "x1 | x2    m1 | m2\n",
        "--------------------\n",
        " 1 | 3     1  |  9\n",
        " 2 | 4     4  | 16\n",
        "\n",
        "(m1 = x1^2, m2 = x2^2)\n",
        "m1 + m2 = score (for w1.w2 = 1)\n",
        "\n",
        "*** 雖然m1+m2=Threshold會是直線(線性),\n",
        "    但轉換為x1^2+x2^2會是圓形(非線性)\n",
        "    <不同角度看會有不同感覺>\n",
        "    <欄位重組, 再做線性分類>\n",
        "\n",
        "*** 我們還是使用線性分類, 但是是因為我們對原始資料進行了轉換(非線性轉換)\n",
        "\n",
        "解決線性分類最好的辦法, 就是把它當成\"多層\"的東西\n",
        "-> Muiti-Layer Perceptron(MLP, 多層感知器)\n",
        "-> x1. x2真的重組出兩個欄位(a1x1+a2x2以及b1x1+b2x2, 等於長出了兩個神經)\n",
        "{前述步驟稱為組合(中間)}\n",
        "\n",
        "-> 再經過Sigmoid函數轉換: sig(Sigma.aixi), 得到m1(同理, sig(Sigma.bixi), 得到m2)\n",
        "{多層感知器的精隨}\n",
        "\n",
        "-> m1. m2丟到Perceptron(賦予加權w1. w2)\n",
        "-> 得到分類器\n",
        "*** 關鍵點: Sigmoid轉換把非線性給加上\n",
        "\n",
        "*** 多層是為了解決線性無法分類的問題\n",
        "*** 多層是因為激活函式才有非線性\n",
        "\n",
        "加權參數要怎麼求? (w1. w2到底該是多少?)\n",
        "第1步: Try and Error(經由嘗試)\n",
        "    -> 隨機你的w1和w2\n",
        "第2步: y'(經由隨機模型, 計算出來的值)\n",
        "第3步: y'跟y靠攏(w1.w2變多或是變少)\n",
        "Ex. w1x1 + w2x2 = y\n",
        "    x1 = 20, x2 = 30, y = 50\n",
        "\n",
        "1.假設w1=5, w2=2\n",
        "2.計算y' = 5*20+2*30 = 160\n",
        "3.y'比y大, 所以w1.w2需要變少\n",
        "3-1. w1改為4.9, w2改為1.9\n",
        "(問題: 靠攏程度到底要多少?\n",
        "-> 損失函式 Loss = y'和y的距離\n",
        "   Loss = (y'-y)^2 )\n",
        "(我們會讓y'=y的時候, 是你的最小值 -> 下山理論)\n",
        "\n",
        "*** 假設隨機一座山, 被綁架丟在山上(沒地圖手機), 走到谷底才能活\n",
        "(故事 -> 數學化)\n",
        "Step. 朝下山的方向走 -> 斜率\n",
        "    <一個東西變動時, 如何影響另一個東西?>\n",
        "    m = delta.y / delta.x\n",
        "    <大小: 坡度, 方向: 往左or往右斜>\n",
        "    x: 選擇(方向) y:不可控(山)\n",
        "    -> 往斜率的反方向走(斜率下降法)\n",
        "*** 決定方向: 正負號\n",
        "    決定大小: 斜率的絕對值\n",
        "    (斜率很大就大步，很小就小步)\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Wx5nvQpuu721"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "問題1:\n",
        "如果在多層感知器最後已經使用Sigmoid來達到類似於Scaling的效果, 那重組階段的激活函式還必須使用Sigmoid嗎?\n",
        "(可否使用類似於x^2之類的非線性函式?)\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "THR0wE5WFPpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "250723_Python_ML_Class08\n",
        "```\n"
      ],
      "metadata": {
        "id": "oX1RCkKt7Ns6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "MLP(多層感知器):\n",
        "Step1. 確定架構(係數隨機)\n",
        "    28x28 黑白圖片, 784個位置, 每個位置有值\n",
        "    0: Black 255:White\n",
        "    x1.x2.x3 ... x784 -> 輸入層\n",
        "    \n",
        "    輸入組合時, 通常會越來越小(784 -> 392 ...)\n",
        "    組合後 -> Sigma.wixi\n",
        "    -> 激活函式轉換(Sigmoid函式)\n",
        "    問題: 要變小多少(幾個神經)? -> 都可以, 你爽就好\n",
        "    老師習慣: 1000 -> 100 -> 10 -> 1\n",
        "    *** 不過, 通常都選2的n次方\n",
        "    784 -> 64 or 128 or 256個\n",
        "    問題簡單: 少一點係數; 問題複雜: 多一點係數\n",
        "    (超參數的選擇憑經驗，但架構大小會根據問題難度而不同)\n",
        "\n",
        "    選128個 -> 組合成m1.m2.m3 ... m128\n",
        "    ** 之前只有0跟1(二元分類)\n",
        "    多種分類該怎麼做? -> 多個輸出\n",
        "        a. 是0的機率(Sigmoid -> 0~1)\n",
        "        b. 是1的機率(Sigmoid -> 0~1)\n",
        "        ...\n",
        "        j. 是9個機率(Sigmoid -> 0~1)\n",
        "\n",
        "Step2. 調整每個係數pred -> true\n",
        "    要先準備一個損失函數(loss):\n",
        "    當你的pred=true時, 必須是最小值\n",
        "    ** 白話文: 你跟正確答案的距離\n",
        "    \n",
        "    最簡單: mse loss (1/n Sigma.(pred-true)^2)\n",
        "\n",
        "    損失函式如何定義?\n",
        "    Ex. 假設正確答案=5 -> 是5的機率=1, 其他=0\n",
        "        -> one-hot encoding\n",
        "    Ex2.假設機率為(0,1,0), 你猜(0.2,0.5,0.3)\n",
        "        -> mse: 1/3 * (0.2-0)^2 + (1-0.5)^2 + (0.3-0)^2\n",
        "\n",
        "Step3. 調整方向: 斜率下降法(負向斜率)\n",
        "    知道斜率 -> 大小:斜率絕對值, 方向:斜率負向\n",
        "    意即 w(加權)的調整 -> 斜率反方向\n",
        "    公式: m(斜率) = delta.y/delta.x\n",
        "         (x變動時, y如何變動?)\n",
        "    \n",
        "    ** 如果今天有方程式, 如何求斜率? -> 微分\n",
        "    Ex. y = x^n\n",
        "        m(斜率) = nx^(n-1)\n",
        "\n",
        "    三次元調整: w1.w2.loss\n",
        "    往哪邊走? ->分成兩個方向\n",
        "    -> 南北往一個方向.東西往一個方向\n",
        "    -> 調w1時, 固定w2, 求delta.loss/delta.w1\n",
        "       (w2的調整同理)\n",
        "\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "S6ad3iYU7SJB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Azj7789jFxpT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}